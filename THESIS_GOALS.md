# ğŸ¯ Thesis Goals: Distributed Edge AI for Robotic Manipulation

**Status**: In Progress (Feb 2026)  
**Target Conference**: Ro-Man 2026  
**Platform**: Franka FR3 + Jetson AGX Orin + ROS 2

---

## High-Level System Architecture (LLM â†’ VLM â†’ Detection â†’ Motion)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   JETSON AGX ORIN (Edge AI)                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM Node (Task Planner)                               â”‚  â”‚
â”‚  â”‚  - Input: /user_command                                â”‚  â”‚
â”‚  â”‚  - Output: /planned_action (JSON plan)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  VLM Agent (Semantic Grounding + Detection)            â”‚  â”‚
â”‚  â”‚  - Input: /vlm_request (image + plan)                  â”‚  â”‚
â”‚  â”‚  - Output: /target_detection (object, pixel, depth)    â”‚  â”‚
â”‚  â”‚  - Provides: label, bbox, pixel coords, depth          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†• ROS 2 Network (pub/sub)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 CONTROLLER PC (Execution)                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RealSense Camera (RGB-D)                              â”‚  â”‚
â”‚  â”‚  - Streams images to Jetson for VLM processing         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Coordinate Transformation Node                        â”‚  â”‚
â”‚  â”‚  - Input: /target_detection (pixel + depth)            â”‚  â”‚
â”‚  â”‚  - Output: /target_pose (x, y, z in robot frame)       â”‚  â”‚
â”‚  â”‚  - Converts pixel+depth â†’ robot coordinates            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Coordinator Node                                      â”‚  â”‚
â”‚  â”‚  - Orchestrates: LLM â†’ VLM â†’ Transform â†’ Motion        â”‚  â”‚
â”‚  â”‚  - Handles user confirmation before execution          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                       â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Motion Execution Layer                                â”‚  â”‚
â”‚  â”‚  - MoveIt 2 + Franka Control                           â”‚  â”‚
â”‚  â”‚  - Publishes: /execution_status                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

UI: Single-page web dashboard (HTML/CSS/JS + jQuery) shows chat, live camera, status, and confirmation.
```

---

## Deployment Split

- **Jetson**: LLM + VLM (with integrated object detection)
- **Controller PC**: Cameras + Coordinate Transform + Coordinator + Motion Execution + UI + Conversation Logger

---

## Implementation Status

### Phase 1: Task Planner (LLM) - **IN PROGRESS**
- [x] Basic LLM node with Ollama integration
- [x] ROS 2 pub/sub setup (subscribes `/user_command`, publishes `/llm_response`)
- [x] System prompt for task planning
- [ ] Context tracking (conversation history)
- [ ] State machine (IDLE â†’ PLANNING â†’ AWAITING_VLM â†’ READY)
- [ ] Timeout handling for VLM/Detection
- [ ] Conversation logging (JSONL)
- [ ] CLI chat node

### Phase 2: VLM Agent (Vision + Detection) - **IN PROGRESS**
- [x] VLM package structure created
- [x] Object detection working (outputs object label, pixel coords, depth)
- [x] Current detection: "red dice" at pixel (870, 270), depth 0.478m
- [ ] Define `/vlm_request` topic (image + task from LLM)
- [ ] Define `/target_detection` topic (object, pixel, depth)
- [ ] Integrate with LLM task planning
- [ ] Handle multiple objects in scene

### Phase 3: Coordinate Transformation - **NOT STARTED** ğŸ”´ (Controller PC)
- [ ] Create transformation node
- [ ] Subscribe to `/target_detection` (pixel + depth from VLM)
- [ ] Subscribe to `/tf` (camera â†’ robot base transform)
- [ ] Subscribe to `/camera_info` (camera intrinsics)
- [ ] Publish `/target_pose` (x, y, z in robot base frame)
- [ ] Test with known object positions
- [ ] Add RViz visualization of detected 3D points

### Phase 4: Motion Execution - **WORKING** âœ… (Controller PC)
- [x] Basic joint control (demo.py on RTX6000)
- [x] MoveIt 2 integration
- [x] Cartesian position control (x, y, z relative to base)
- [x] Gripper control (open/close)
- [x] Fixed gripper orientation for grasping (rx=0, ry=Ï€, rz=2.45)
- [x] Velocity scaling control
- [ ] Pick & place primitives (grasp sequence)
- [ ] Camera-to-robot coordinate transformation
- [ ] Perception-aware planning (use target detection + 3D map)
- [ ] Feedback loop (`/execution_status`) to coordinator
Transform â†’ Motion orchestration
- [ ] Subscribes: `/planned_action`, `/target_detection`, `/target_pose
- [ ] LLM â†’ VLM â†’ YOLO â†’ Motion orchestration
- [ ] Subscribes: `/planned_action`, `/vlm_grounding`, `/target_detection`
- [ ] Publishes: `/execution_status`, `/execution_goal`
- [ ] Requires user confirmation before execution
- [ ] Error recovery and timeout handling

### Phase 6: UI (Single-Page Dashboard) - **NOT STARTED** (Controller PC)
- [ ] Chat panel (all agents)
- [ ] User input box
- [ ] Live camera feed
- [ ] Status panel
- [ ] Confirmation step before execution
- [ ] Chat starts with assistant greeting (e.g., "Hi, I'm Franka, your assistant")
- [ ] Every agent/status log appears in the chat timeline

---

## Thesis Objectives (Updated)

### Primary Goal
**Distributed Edge AI Architecture for Robotic Pick & Place**

Implement and evaluate a distributed system where:
1. **LLM** (Task Planner) generates high-level intent
2. **VLM** (Vision + Detection) identifies and localizes the target object
3. **Coordinate Transform** converts pixel+depth â†’ robot coordinates
4. **Motion Execution** handles low-level control
5. **Coordinator** enforces safety, confirmation, and orchestration

### Multi-Agent & Conversation Logging Goal
- All inter-agent messages (user â†’ LLM â†’ VLM â†’ Coordinator â†’ Motion) are logged as a chat-like transcript.
- UI shows full agent chat + execution status + confirmation step.
- UI starts with an assistant greeting and keeps a full timeline of logs.

### Task Evaluation
- [ ] Implement 3-5 pick & place scenarios
- [ ] Compare LLM-only, VLM-only, and Hybrid (LLM+VLM+Detection)
- [ ] Metrics: success rate, latency, safety accuracy, human confirmation rate

---

**Last Updated**: 2026-02-10 15:44:00  
**Authored By**: Arash